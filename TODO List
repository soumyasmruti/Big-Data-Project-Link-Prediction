TODO List: (To be updated regularly)

One very imporatant thing to be considered. Please make sure the name of the PySpark files you create are not the same as that in
the source code repo. Otherwise it will be easy for him to do a search and find out. Name the files like how you would and try to make
it as different as possible from the source code files.


1. Cleaned up dataset ( The json folder for hotelreviews has the text and other information)
I remember you telling me you had cleaned up the input json folder to contain only review info which is necessary. Please push that 
folder of cleaned up review file here.

<TODO Push the cleaned up input data files into the repo>

2. Create user.json, reviews.json and hotels.json using Spark and push them in here once done.

While creating user.json the following fields should be there:
i. unique user_id generated by us for each user based on the username
ii. The date of his last review
iii. The total no of reviews or review count
iv. There is also the location field in few of the user reviews. That field is called "AuthorLocation". Store that for each user too.
We might use it later.

This count of users is after filtering users who have written 
more than 1000 or some threshold amount of reviews. Remember the discussion or the slide we had in our presentation. We told
them we have removed certain generic users who had very high review count so that data isn't skewed. Make sure that filtering is done



While creating hotels.json, the following fields should be there:
i. unique hotel_id generated by us for each hotel based on the hotel id
ii. The total no of reviews received.
iii. rating
iv. location

While creating reviews.json, the following fields should be there:
i.user_id
ii.hotel_id
iii. date of review
iv. ratings


<TODO Push the newly created user.json, hotels.json and review.json >


At the end of this also note down the total count of hotels and total count of users.

<TODO Please put down the total number of hotels and total number of users here>

Essentially this will be our full adjacency
matrix. (u x h ). He wanted us to check if this uxh can fit in memory or not too large, then doing svd and stuff  might not be 
relevant. So make sure to somehow get the count of users and hotels. 

3. Create graph.txt based on the userid and hotelid and push the graph.txt file here.
Once you have created the graph.txt we can do svd on it using Hari Sundar's code. he told me yesterday he had a python code for 
doing svd using stochastic gradient descent

Link to his Python code: http://www.cs.utah.edu/~hari/teaching/bigdata/uv_decomp.html

Convert this code to Spark and run it for our graph. Tell me what you get as results

<TODO Results of decomposition here or attach a text file with results >

4. Creating examples and storing them. Understand what the code is doing and do it. When picking examples,
there are two conditions given in the report based on which they did the filtering:

a. Only taking users that have been active (i.e. that have written a review) in the last six months.This removes 12% of positive examples and 62% of negative examples.
b. Only taking businesses at distance 3 from the current user as candidate edges. This further removes 22% of positive examples and 66% of negative examples.

Make sure to do this before creating examples. Again I guess the existing code might have it but make sure you understand what
is required.

Be very clear about the test and train examples. Tell me what dates you are using for test and train here. Please update it once 
you have done that here :

<TODO Complete the dates> 


5. Similarity code (Should be in Spark. Nothing else would suffice)
Do it for two similarity metrics a) Jaccard  and b)Common neighbors

For similarity they are doing it for both sides of the bi-partite graph. i.e they do it for businness as well as users. So for 
each similarity they are creating two files. Also they are doing it for both test and train, so in total 4 files.

6. Random walks. Random walks is similar to the page rank code. Again for this one they would have used something different. But
we can say we used our page rank code and try to get some values.

7. Gradient boosting. Even if you can find libraries for it in non-Spark python code, use it and we will state that in report.
But we should definitely have one of these supervised classsification techniques code. Cannot compromise on this one.
