{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataRDD = sc.wholeTextFiles(\"data/xmlfiles/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def xml_parsing(fileName):\n",
    "    \n",
    "    text_content = fileName.encode('utf-8')\n",
    "\n",
    "    # Regular expressions to find all the links and text from XML files and storing them in two lists\n",
    "    TITLE_RE = re.compile(r'<title>\\s*(.+)\\s*<\\/title>')\n",
    "    LINK_RE = re.compile(r'\\[\\[([^\\]]+)\\]')\n",
    "    TEXT_RE = re.compile(r'<text.+>\\s*(.+)\\s*<\\/text>')\n",
    "\n",
    "    #print 'Links:' , LINK_RE.findall(text_content)\n",
    "    #print 'Text: ', TEXT_RE.findall(text_content)\n",
    "    title = TITLE_RE.findall(text_content)\n",
    "    #Creating a tuple of links, text where each of them is a list\n",
    "    tup = (LINK_RE.findall(text_content), TEXT_RE.findall(text_content))\n",
    "\n",
    "    return \" \".join(title), tup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contents = dataRDD.values().map(xml_parsing)\n",
    "linkRDD = contents.flatMap(lambda x: [x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultRDD= linkRDD.zip(dataRDD.map(lambda x : x[0]))\n",
    "#resultRDD.map(lambda x: (x[0][1],x[0][0],x[1]))\n",
    "linkList = sorted(resultRDD.collect())\n",
    "linked={}\n",
    "for i, j in enumerate(linkList):\n",
    "    linked[j[1]]= (i, j[0])\n",
    "# print 'Link is:', link    \n",
    "import json\n",
    "with open('PageTitles.json', 'w') as fp:\n",
    "    json.dump(linked, fp)\n",
    "    \n",
    "linkListMain = sorted(linkRDD.collect())\n",
    "link = {j:i for i, j in enumerate(linkListMain)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "nVector = [(0.15/len(link))]*len(link)\n",
    "def documentTermVector(i, fileLinks):\n",
    "    vecList = [0.0]*len(link)\n",
    "    m = 0\n",
    "    print len(fileLinks)\n",
    "    for each in fileLinks:\n",
    "        if \"|\" in each:\n",
    "            x = each.split(\"|\")[0]\n",
    "            try:\n",
    "                if(link[x]):\n",
    "                    m += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                print each\n",
    "                if(link[each]+1):\n",
    "                    print m\n",
    "                    m += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "    print m\n",
    "    for each in fileLinks:\n",
    "        if \"|\" in each:\n",
    "            x = each.split(\"|\")[0]\n",
    "            try:\n",
    "                vecList[link[x]] += 0.85/m\n",
    "            except KeyError:\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                vecList[link[each]] += 0.85/m\n",
    "            except KeyError:\n",
    "                continue\n",
    "    tempList = map(operator.add, vecList,nVector)\n",
    "    finallist = [[link[i], s, r] for s, r in enumerate(tempList)]\n",
    "    return finallist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdmMatrix = contents.flatMap(lambda x: documentTermVector(x[0], x[1][0]))\n",
    "colMat = tdmMatrix.map(lambda x: (x[1], [x[2]])).reduceByKey(lambda p,q:p+q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomSurfer = sc.parallelize([(1.0/len(link))]*len(link)).map(lambda x: (0, [x])).reduceByKey(lambda p,q:p+q)\n",
    "#randomSurfer.collect()\n",
    "# tdmMatrix.take(2)\n",
    "# documentTermVector(1, ['Amuck',\n",
    "#                         'Amuck',                                           \n",
    "# 'Amuck','Amuck',\n",
    "#  'Symphony No. 9 (Beethoven)',\n",
    "#  'Running amok',\n",
    "#  'BiblicalInterpretation',\n",
    "#  'Abiword',\n",
    "#  'BirthofaNation',\n",
    "#  'RUR-5 ASROC',\n",
    "#  'Alexandria Troas',\n",
    "#  'Benjamin Lee Whorf',\n",
    "#  'Amnon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cartRDD = colMat.cartesian(randomSurfer)\\\n",
    "                .map(lambda x: np.dot(x[0][1], x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cart2RDD = colMat.cartesian(cartRDD.map(lambda x: (0, [x])).reduceByKey(lambda p,q:p+q))\\\n",
    "                .map(lambda x: np.dot(x[0][1], x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normRdd = cartRDD.map(lambda x: (0, [x]))\\\n",
    "#                  .reduceByKey(lambda p,q:p+q)\\\n",
    "#                  .cartesian(cart2RDD.map(lambda x: (0, [x])).reduceByKey(lambda p,q:p+q))\\\n",
    "#                  .map(lambda x: np.linalg.norm((np.array(x[0][1]) - np.array(x[1][1])), ord = 2))\n",
    "\n",
    "# normRdd.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pageRankDict = {}\n",
    "def pageRank(vprime, vnewprime, c):\n",
    "    c+=1\n",
    "    norm = []\n",
    "    norm = vprime.map(lambda x: (0, [x]))\\\n",
    "                 .reduceByKey(lambda p,q:p+q)\\\n",
    "                 .cartesian(vnewprime.map(lambda x: (0, [x])).reduceByKey(lambda p,q:p+q))\\\n",
    "                 .map(lambda x: np.linalg.norm((np.array(x[0][1]) - np.array(x[1][1])), ord = 2)).collect()\n",
    "    n = float(norm[0])\n",
    "    print \"Norm is: \", c, n, type(n)\n",
    "    if n <= 0.000001:\n",
    "        pageRankList = vnewprime.collect()\n",
    "        for i, j in enumerate(pageRankList):\n",
    "            pageRankDict[i]=j\n",
    "        with open('PageRankResults.json', 'w' ) as fp:\n",
    "            json.dump(pageRankDict, fp)\n",
    "        return pageRankList\n",
    "    else:\n",
    "        vNew = colMat.cartesian(vnewprime.map(lambda x: (0, [x]))\\\n",
    "                     .reduceByKey(lambda p,q:p+q))\\\n",
    "                     .map(lambda x: np.dot(x[0][1], x[1][1]))\n",
    "        pageRank(vnewprime, vNew, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm is:  1 0.0290058577432 <type 'float'>\n",
      "Norm is:  2 0.0136369135139 <type 'float'>\n",
      "Norm is:  3 0.00868227558324 <type 'float'>\n",
      "Norm is:  4 0.00344827039967 <type 'float'>\n",
      "Norm is:  5 0.00199242046689 <type 'float'>\n",
      "Norm is:  6 0.00125518706068 <type 'float'>\n",
      "Norm is:  7 0.000763624222336 <type 'float'>\n",
      "Norm is:  8 0.000475836575444 <type 'float'>\n",
      "Norm is:  9 0.000290376885166 <type 'float'>\n",
      "Norm is:  10 0.000180455621675 <type 'float'>\n",
      "Norm is:  11 0.000110318041291 <type 'float'>\n",
      "Norm is:  12 6.8450449741e-05 <type 'float'>\n",
      "Norm is:  13 4.18745274133e-05 <type 'float'>\n",
      "Norm is:  14 2.59670352321e-05 <type 'float'>\n",
      "Norm is:  15 1.58928053764e-05 <type 'float'>\n",
      "Norm is:  16 9.85212113048e-06 <type 'float'>\n",
      "Norm is:  17 6.03226584475e-06 <type 'float'>\n",
      "Norm is:  18 3.73853247895e-06 <type 'float'>\n",
      "Norm is:  19 2.28986661633e-06 <type 'float'>\n",
      "Norm is:  20 1.41884083711e-06 <type 'float'>\n",
      "Norm is:  21 8.6934346973e-07 <type 'float'>\n"
     ]
    }
   ],
   "source": [
    "pg = pageRank(cartRDD, cart2RDD, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link[\"Brazil\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_parsing(fileName):\n",
    "    text_content = fileName.encode('utf-8')\n",
    "\n",
    "    # Regular expressions to find all the links and text from XML files and storing them in two lists\n",
    "    TEXT_RE = re.compile(r'<text.+>([\\s\\S]*)<\\/text>')\n",
    "    \n",
    "    liste = TEXT_RE.findall(text_content)\n",
    "    str1 = re.split('[^a-zA-Z.]', liste[0].lower())\n",
    "    str2 = filter (None, str1)\n",
    "    return str2\n",
    "\n",
    "splitRDD = dataRDD.values().map(text_parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(splitRDD)\n",
    "from pyspark.mllib.feature import IDF\n",
    "\n",
    "# ...from tf create IDF\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63556\n"
     ]
    }
   ],
   "source": [
    "zipped = splitRDD.zip(tfidf)\n",
    "fRDD = splitRDD.flatMap(lambda x: x).distinct()\n",
    "print fRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordRDD = fRDD.map(lambda x: (x, hashingTF.indexOf(x)))\n",
    "listW = wordRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "dictW = dict(listW)\n",
    "\n",
    "with open(\"wordHashD.json\", 'w') as f:\n",
    "    json.dump(dictW, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalZip = dataRDD.map(lambda x: x[0]).zip(zipped)\n",
    "# finalZip.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def searchWords(words):\n",
    "    words = words.lower()\n",
    "    resultList = []\n",
    "    for word in words.split(\" \"):\n",
    "        resultList.append(finalZip.map(lambda x: (x[0], x[1][0], x[1][1]))\\\n",
    "                     .filter(lambda x: word in x[1])\\\n",
    "                     .map(lambda x: (x[2][dictW[word]], x[0]))\\\n",
    "                     .map(lambda x : (x[1], pageRankDict[linked[x[1]][0]], x[0], linked[x[1]][1]))\\\n",
    "                     .sortBy(lambda x: x[1], False)\\\n",
    "                     .sortBy(lambda x: x[2], False).take(5))\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'file:/home/vagrant/data/xmlfiles/page-0001961.xml',\n",
       "   3.6173057647550886e-09,\n",
       "   368.05060426715016,\n",
       "   'Black Forest'),\n",
       "  (u'file:/home/vagrant/data/xmlfiles/page-0001962.xml',\n",
       "   3.4946785748791316e-08,\n",
       "   278.60404307075112,\n",
       "   'Black Sea'),\n",
       "  (u'file:/home/vagrant/data/xmlfiles/page-0001818.xml',\n",
       "   8.6398269018719571e-09,\n",
       "   30.793078444661965,\n",
       "   'Atlanta'),\n",
       "  (u'file:/home/vagrant/data/xmlfiles/page-0001928.xml',\n",
       "   8.6398269018719571e-09,\n",
       "   29.32674137586854,\n",
       "   'The Birth of a Nation'),\n",
       "  (u'file:/home/vagrant/data/xmlfiles/page-0001781.xml',\n",
       "   1.1477225206606812e-08,\n",
       "   24.927730169488257,\n",
       "   'Alexandria')],\n",
       " [(u'file:/home/vagrant/data/xmlfiles/page-0001961.xml',\n",
       "   3.6173057647550886e-09,\n",
       "   654.0620861526919,\n",
       "   'Black Forest'),\n",
       "  (u'file:/home/vagrant/data/xmlfiles/page-0001783.xml',\n",
       "   3.6173057647550886e-09,\n",
       "   23.08454421715383,\n",
       "   'Alexandria, Louisiana'),\n",
       "  (u'file:/home/vagrant/data/xmlfiles/page-0001960.xml',\n",
       "   7.3685201693453462e-07,\n",
       "   17.954645502230758,\n",
       "   'Brazil'),\n",
       "  (u'file:/home/vagrant/data/xmlfiles/page-0001894.xml',\n",
       "   3.6173057647550886e-09,\n",
       "   15.389696144769221,\n",
       "   'Acid rain'),\n",
       "  (u'file:/home/vagrant/data/xmlfiles/page-0001818.xml',\n",
       "   8.6398269018719571e-09,\n",
       "   12.824746787307683,\n",
       "   'Atlanta')]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchWords(\"Black Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
